# ğŸ§  TinyGrad

> A minimal deep learning framework built from scratch for educational purposes

**TinyGrad** is a clean-room implementation of a neural network framework in pure Python and NumPy/CuPy, demonstrating the core concepts behind modern deep learning libraries like PyTorch and TensorFlow.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## âœ¨ Key Features

- ğŸ”¥ **Automatic Differentiation** - Full reverse-mode autograd engine with dynamic computation graphs
- ğŸš€ **GPU Acceleration** - CUDA support via CuPy with optimized convolution kernels
- ğŸ“Š **Common Layers** - Linear, Conv2d, BatchNorm, Dropout, Pooling, and more
- ğŸ¯ **Modern Optimizers** - SGD, Adam, AdamW with learning rate scheduling
- ğŸ§® **Mixed Precision Training** - FP16 automatic mixed precision for faster training
- ğŸ“ˆ **Real Datasets** - MNIST and CIFAR-10 loaders with data augmentation
- ğŸ—ï¸ **Modern Architectures** - MLP, CNN, ResNet implementations
- ğŸ¨ **Interactive Visualizer** - Streamlit-based neural network visualization tool

## ğŸš€ Performance Benchmarks

### CIFAR-10 ResNet-20 Training Performance

Trained on **NVIDIA GTX 1050 Ti** (4GB VRAM) for 2000 steps (batch size 64).

### GPU Optimization: im2col vs Stride Tricks

<details>
<summary>Click to expand</summary>

**Problem:** Convolution is expensive (millions of operations)

**Naive approach (your baseline):**
```python
# Python loops - SLOW!
for i in range(kernel_height):
    for j in range(kernel_width):
        output += input_patch[i,j] * kernel[i,j]
```
Time: **459ms** per iteration

**Optimization 1: im2col + matrix multiply**
```python
# Convert to matrix multiply (faster but copies data)
patches = im2col(input)  # Create matrix of all patches
output = patches @ kernel_flat
```
Time: Still ~459ms (loop overhead + copying)

**Optimization 2: Stride tricks (zero-copy)**
```python
# Create VIEW of data (no copying!)
strides = (stride_h, stride_w, ...)
patches = np.lib.stride_tricks.as_strided(input, shape, strides)
output = patches @ kernel_flat
```
Time: **27ms** per iteration (**16.87Ã— faster!**)

**Why so much faster?**
- âœ… No Python loops (single NumPy/CuPy operation)
- âœ… No data copying (380ms saved!)
- âœ… Better GPU memory access patterns
- âœ… Leverages optimized cuBLAS for matrix multiply

</details>

### Memory Management

<details>
<summary>Click to expand</summary>

**GPU memory is limited!** (my GTX 1050 Ti has only 4GB)

**Strategies implemented:**
1. **In-place operations** where possible
2. **Memory pooling** - Reuse allocated memory
3. **Periodic cache clearing** - Free unused memory every 10 steps
4. **Mixed precision (FP16)** - 50% less memory
5. **Gradient accumulation** - Train with larger effective batch sizes

## ğŸ¯ Learning Objectives

This project demonstrates:

1. **Automatic Differentiation** - How autograd engines work under the hood
2. **GPU Programming** - CUDA acceleration with CuPy
3. **Performance Optimization** - Memory management, vectorization, kernel fusion
4. **Neural Network Architectures** - MLP, CNN, ResNet from scratch
5. **Training Dynamics** - Optimizers, learning rate schedules, regularization
6. **Software Engineering** - Clean API design, testing, documentation
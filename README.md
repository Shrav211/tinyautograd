# üß† TinyGrad

> A minimal deep learning framework built from scratch for educational purposes

**TinyGrad** is a clean-room implementation of a neural network framework in pure Python and NumPy/CuPy, demonstrating the core concepts behind modern deep learning libraries like PyTorch and TensorFlow.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ‚ú® Key Features

- üî• **Automatic Differentiation** - Full reverse-mode autograd engine with dynamic computation graphs
- üöÄ **GPU Acceleration** - CUDA support via CuPy with optimized convolution kernels
- üìä **Common Layers** - Linear, Conv2d, BatchNorm, Dropout, Pooling, and more
- üéØ **Modern Optimizers** - SGD, Adam, AdamW with learning rate scheduling
- üßÆ **Mixed Precision Training** - FP16 automatic mixed precision for faster training
- üìà **Real Datasets** - MNIST and CIFAR-10 loaders with data augmentation
- üèóÔ∏è **Modern Architectures** - MLP, CNN, ResNet implementations
- üé® **Interactive Visualizer** - Streamlit-based neural network visualization tool

## üöÄ Performance Benchmarks

### CIFAR-10 ResNet-20 Training Performance

Trained on **NVIDIA GTX 1050 Ti** (4GB VRAM) for 2000 steps (batch size 64).

### GPU Optimization: im2col vs Stride Tricks

<details>
<summary>Click to expand</summary>

**Problem:** Convolution is expensive (millions of operations)

**Naive approach (your baseline):**
```python
# Python loops - SLOW!
for i in range(kernel_height):
    for j in range(kernel_width):
        output += input_patch[i,j] * kernel[i,j]
```
Time: **459ms** per iteration

**Optimization 1: im2col + matrix multiply**
```python
# Convert to matrix multiply (faster but copies data)
patches = im2col(input)  # Create matrix of all patches
output = patches @ kernel_flat
```
Time: Still ~459ms (loop overhead + copying)

**Optimization 2: Stride tricks (zero-copy)**
```python
# Create VIEW of data (no copying!)
strides = (stride_h, stride_w, ...)
patches = np.lib.stride_tricks.as_strided(input, shape, strides)
output = patches @ kernel_flat
```
Time: **27ms** per iteration (**16.87√ó faster!**)

**Why so much faster?**
- ‚úÖ No Python loops (single NumPy/CuPy operation)
- ‚úÖ No data copying (380ms saved!)
- ‚úÖ Better GPU memory access patterns
- ‚úÖ Leverages optimized cuBLAS for matrix multiply

</details>

### Memory Management

<details>
<summary>Click to expand</summary>

**GPU memory is limited!** (my GTX 1050 Ti has only 4GB)

**Strategies implemented:**
1. **In-place operations** where possible
2. **Memory pooling** - Reuse allocated memory
3. **Periodic cache clearing** - Free unused memory every 10 steps
4. **Mixed precision (FP16)** - 50% less memory
5. **Gradient accumulation** - Train with larger effective batch sizes

## üéØ Learning Objectives

This project demonstrates:

1. **Automatic Differentiation** - How autograd engines work under the hood
2. **GPU Programming** - CUDA acceleration with CuPy
3. **Performance Optimization** - Memory management, vectorization, kernel fusion
4. **Neural Network Architectures** - MLP, CNN, ResNet from scratch
5. **Training Dynamics** - Optimizers, learning rate schedules, regularization
6. **Software Engineering** - Clean API design, testing, documentation

‚≠ê **Star this repo if you found it helpful!**

Built with ‚ù§Ô∏è for learning deep learning from first principles

## üôè Acknowledgments

- Inspired by [PyTorch](https://pytorch.org/), [micrograd](https://github.com/karpathy/micrograd), and [tinygrad](https://github.com/tinygrad/tinygrad)
- CIFAR-10 dataset from [Alex Krizhevsky](https://www.cs.toronto.edu/~kriz/cifar.html)
- GPU optimizations based on [CuPy documentation](https://docs.cupy.dev/)
- Educational resources from [fast.ai](https://www.fast.ai/) and [Dive into Deep Learning](https://d2l.ai/)

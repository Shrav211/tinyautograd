# tinygrad/nn_cupy_optimized.py
"""
Optimized convolution using CuPy's native functions
No cuDNN required - uses CuPy's built-in optimizations
"""
import numpy as np
try:
    import cupy as cp
    CUPY_AVAILABLE = True
except ImportError:
    CUPY_AVAILABLE = False
    cp = None

from tinygrad.tensor import Tensor


class Conv2dCuPyOptimized:
    """
    Fast convolution using CuPy's optimized im2col
    
    5-10Ã— faster than manual im2col, no cuDNN needed!
    """
    
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):
        if not CUPY_AVAILABLE:
            raise ImportError("CuPy not available")
        
        self.in_channels = in_channels
        self.out_channels = out_channels
        
        # Handle kernel_size as int or tuple
        if isinstance(kernel_size, int):
            self.kernel_size = (kernel_size, kernel_size)
        else:
            self.kernel_size = kernel_size
        
        # Handle stride and padding
        if isinstance(stride, int):
            self.stride = (stride, stride)
        else:
            self.stride = stride
            
        if isinstance(padding, int):
            self.padding = (padding, padding)
        else:
            self.padding = padding
        
        # Initialize weights: (out_channels, in_channels, kH, kW)
        kH, kW = self.kernel_size
        scale = np.sqrt(2.0 / (in_channels * kH * kW))
        
        weight_data = cp.random.randn(out_channels, in_channels, kH, kW).astype(cp.float32) * scale
        self.W = Tensor(weight_data, requires_grad=True)
        
        # Initialize bias
        self.use_bias = bias
        if bias:
            self.b = Tensor(cp.zeros(out_channels, dtype=cp.float32), requires_grad=True)
        else:
            self.b = None
    
    def _im2col_cupy(self, x):
        """
        Optimized im2col using CuPy's get_array_module and stride_tricks
        
        This is much faster than the manual Python loop version!
        """
        N, C, H, W = x.shape
        kH, kW = self.kernel_size
        pH, pW = self.padding
        sH, sW = self.stride
        
        # Pad input
        if pH > 0 or pW > 0:
            x_pad = cp.pad(x, ((0, 0), (0, 0), (pH, pH), (pW, pW)), mode='constant')
        else:
            x_pad = x
        
        H_pad, W_pad = x_pad.shape[2], x_pad.shape[3]
        out_h = (H_pad - kH) // sH + 1
        out_w = (W_pad - kW) // sW + 1
        
        # Use CuPy's stride_tricks (optimized for GPU)
        # This creates a view without copying data
        shape = (N, C, kH, kW, out_h, out_w)
        
        strides = x_pad.strides
        strides = (
            strides[0],      # N
            strides[1],      # C
            strides[2],      # kH
            strides[3],      # kW
            strides[2] * sH, # out_h (stride in H)
            strides[3] * sW  # out_w (stride in W)
        )
        
        # Create strided view
        col = cp.lib.stride_tricks.as_strided(x_pad, shape=shape, strides=strides)
        
        # Reshape to (N * out_h * out_w, C * kH * kW)
        col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, C * kH * kW)
        
        return col, out_h, out_w
    
    def _col2im_cupy(self, col, x_shape, out_h, out_w):
        """
        Optimized col2im for gradient computation
        """
        N, C, H, W = x_shape
        kH, kW = self.kernel_size
        pH, pW = self.padding
        sH, sW = self.stride
        
        H_pad = H + 2 * pH
        W_pad = W + 2 * pW
        
        # Reshape col
        col = col.reshape(N, out_h, out_w, C, kH, kW).transpose(0, 3, 4, 5, 1, 2)
        
        # Initialize padded output
        dx_pad = cp.zeros((N, C, H_pad, W_pad), dtype=col.dtype)
        
        # Accumulate gradients
        for i in range(kH):
            i_max = i + sH * out_h
            for j in range(kW):
                j_max = j + sW * out_w
                dx_pad[:, :, i:i_max:sH, j:j_max:sW] += col[:, :, i, j, :, :]
        
        # Remove padding
        if pH == 0 and pW == 0:
            return dx_pad
        return dx_pad[:, :, pH:-pH, pW:-pW]
    
    def __call__(self, x):
        """
        Forward pass using optimized CuPy im2col
        """
        # Ensure input is on GPU
        if not isinstance(x.data, cp.ndarray):
            raise ValueError("Input must be on GPU (cupy array)")
        
        N, C, H, W = x.data.shape
        kH, kW = self.kernel_size
        
        # im2col (optimized)
        X_col, out_h, out_w = self._im2col_cupy(x.data)
        
        # Reshape weights for matrix multiply
        W_col = self.W.data.reshape(self.out_channels, -1).T  # (C*kH*kW, F)
        
        # Matrix multiply
        out_col = X_col @ W_col  # (N*out_h*out_w, F)
        
        # Reshape to output image
        output_data = out_col.reshape(N, out_h, out_w, self.out_channels).transpose(0, 3, 1, 2)
        
        # Add bias if present
        if self.use_bias:
            bias_reshaped = self.b.data.reshape(1, self.out_channels, 1, 1)
            output_data = output_data + bias_reshaped
        
        # --- FIX: Respect no_grad context! ---
        # Only require grad if global flag is True AND inputs require grad.
        # This prevents storing the massive X_col during evaluation.
        should_require_grad = Tensor._grad_enabled and (x.requires_grad or self.W.requires_grad)
        
        out = Tensor(output_data, requires_grad=should_require_grad)
        
        # Setup backward pass (only if needed)
        if out.requires_grad:
            out._prev = {x, self.W}
            if self.use_bias:
                out._prev.add(self.b)
            out._op = "conv2d_cupy_optimized"
            
            # Store for backward
            self._x_col = X_col
            self._x_shape = x.data.shape
            self._out_h = out_h
            self._out_w = out_w
            
            def _backward():
                if out.grad is None:
                    return
                
                dy = out.grad # Raw array
                
                # 1. Gradient w.r.t Input (x)
                if x.requires_grad:
                    if hasattr(x, "_Tensor__init_grad"): x._Tensor__init_grad()
                    else: x.__init_grad()
                        
                    dout = dy.transpose(0, 2, 3, 1).reshape(N * out_h * out_w, self.out_channels)
                    W_col = self.W.data.reshape(self.out_channels, -1).T
                    dX_col = cp.dot(dout, W_col.T)
                    dx_cupy = self._col2im_cupy(dX_col, self._x_shape, out_h, out_w)
                    
                    x.grad += dx_cupy
                
                # 2. Gradient w.r.t Weights (W)
                if self.W.requires_grad:
                    if hasattr(self.W, "_Tensor__init_grad"): self.W._Tensor__init_grad()
                    else: self.W.__init_grad()
                    
                    dout = dy.transpose(0, 2, 3, 1).reshape(N * out_h * out_w, self.out_channels)
                    dW_col = cp.dot(self._x_col.T, dout)
                    dW_cupy = dW_col.T.reshape(self.out_channels, C, kH, kW)
                    
                    self.W.grad += dW_cupy
                
                # 3. Gradient w.r.t Bias (b)
                if self.use_bias and self.b.requires_grad:
                    if hasattr(self.b, "_Tensor__init_grad"): self.b._Tensor__init_grad()
                    else: self.b.__init_grad()
                    
                    db_cupy = cp.sum(dy, axis=(0, 2, 3))
                    self.b.grad += db_cupy
            
            out._backward = _backward
        
        return out
    
    def parameters(self):
        """Return list of parameters for optimizer"""
        params = [self.W]
        if self.use_bias:
            params.append(self.b)
        return params
    
    def to(self, device):
        """Move layer to device"""
        self.W = self.W.to(device)
        if self.use_bias:
            self.b = self.b.to(device)
        return self